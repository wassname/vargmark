===
title: "Should We Pause AI Development at 150% of Human-Level Capability?"
author: wassname
date: 2026-02-10
abstract: >
  A structured argument map analyzing the proposal to pause AI development
  once systems reach 150% of human-level capability. Presents the case for
  the pause (existential risk, alignment unsolved, precautionary principle)
  and against (enforcement, opportunity costs, competitive dynamics, threshold
  arbitrariness). Evaluates which side is stronger and why.
map:
  statementLabelMode: text
  argumentLabelMode: title
  groupDepth: 3
model:
  mode: strict
color:
  colorScheme: colorbrewer-set3
  tagColors:
    pro: "#4CAF50"
    con: "#f44336"
    observation: "#2196F3"
    assumption: "#FF9800"
    meta: "#9C27B0"
===

// ==========================================================================
// CENTRAL CLAIM
// ==========================================================================

# The Proposal

[Pause150]: We should impose a mandatory pause on AI development once systems
reach 150% of human-level capability across cognitive tasks, not to be lifted
until alignment and governance are demonstrably adequate. #meta

// ==========================================================================
// SUPPORTING SIDE
// ==========================================================================

# Arguments For the Pause {isGroup: true}

## Existential Risk Is Real {isGroup: true}

[ExpertWarning]: Leading AI researchers -- including Turing Award winners
Bengio and Hinton -- warn that advanced AI poses existential risk to humanity
comparable to pandemics and nuclear war. #observation
  > "Mitigating the risk of extinction from AI should be a global priority
  > alongside other societal-scale risks such as pandemics and nuclear war."
  [CAIS Statement on AI Risk, signed by Bengio, Hinton, Altman et al.](https://www.safe.ai/statement-on-ai-risk)
  {credence: 0.80, reason: "Signed by hundreds of leading researchers and lab CEOs. High credibility but incentive mix is complex -- some signatories benefit from regulation that locks in incumbents."}

[ExpertSurvey]: A 2023 survey of ML researchers found a median estimate of
at least 5% probability that AGI leads to human extinction or permanent
disempowerment, with a mean estimate of ~14%. #observation
  > "The median respondent estimated that there was a 50% chance of AGI by
  > 2047 and that there was at least a 5% chance AGI would result in an
  > existential catastrophe."
  [Grace et al., 2024. Thousands of AI Authors on the Future of AI.](https://arxiv.org/abs/2401.02843)
  {credence: 0.70, reason: "Large-sample survey (~2800 respondents) but self-selected from NeurIPS/ICML authors. Estimates are subjective and vary enormously across individuals."}

[NoSlowdown]: There is no fundamental reason AI progress would halt at human
level. AI systems can be copied, run faster, and scaled to use vastly more
compute than a human brain. #observation
  > "There is no fundamental reason why AI progress would slow or halt when
  > it reaches human-level abilities. Compared to humans, AI systems can act
  > faster, absorb more knowledge, and communicate at a far higher bandwidth."
  [Bengio, Hinton et al. Managing extreme AI risks amid rapid progress. Science, 2024.](https://doi.org/10.1126/science.adn0117)
  {credence: 0.85, reason: "Published in Science by domain founders. The claim is about what's possible, not what will happen -- absence of a barrier is well-supported."}

<ExistentialRiskArg>: The risk of extinction from advanced AI is nontrivial
and acknowledged by domain experts. #pro
  + [Pause150]

<ExistentialRiskArg>

(1) [ExpertWarning]
(2) [ExpertSurvey]
----
(3) [NonTrivialRisk]: The probability of existential catastrophe from advanced
AI is nontrivial (expert median >= 5%). #observation
  {inference: 0.75}

<FOOMRisk>: Beyond human-level, AI capabilities may escalate uncontrollably
via recursive self-improvement, making the window for intervention very short.
#pro
  + [Pause150]

<FOOMRisk>

(1) [NoSlowdown]
(2) [RecursiveSelfImprovement]: A sufficiently capable AI system could improve
its own training, architecture, or data pipeline, leading to a rapid capability
gain ("intelligence explosion"). #assumption
  {credence: 0.50, reason: "Theoretical argument from Good (1965) and Bostrom (2014). No empirical examples yet. Current AI can assist in ML research but full recursive loop is undemonstrated."}
----
(3) [IntelligenceExplosion]: Once AI exceeds human cognitive ability, recursive
self-improvement could produce a rapid jump to far-superhuman capability. #assumption
  {inference: 0.40}
(4) [NarrowWindow]: The window between 'slightly superhuman' and 'far
superhuman' may be too short for human intervention. #assumption
  {credence: 0.35, reason: "Speculative. Depends on speed of recursive improvement, which could be fast (Bostrom) or bottlenecked by compute/data/validation. Low but nonzero."}
----
(5) [FOOMConclusion]: A pause at 150% gives a safety margin before potential
recursive takeoff eliminates the ability to intervene. #assumption
  {inference: 0.30}
  +> [Pause150]


## Alignment Is Unsolved {isGroup: true}

[AlignmentHard]: The technical problem of reliably aligning AI systems with
human values is unsolved, and leading researchers say current methods do not
scale to superhuman systems. #observation
  > "No one currently knows how to reliably align AI behavior with complex
  > values. Rather than treating safety as a secondary consideration, it should
  > be a central focus."
  [Bengio, Hinton et al. Managing extreme AI risks amid rapid progress. Science, 2024.](https://doi.org/10.1126/science.adn0117)
  {credence: 0.85, reason: "Consensus statement from the field's founders. RLHF works for current systems but its adequacy for superhuman systems is undemonstrated."}

[AltmanAdmits]: Even the CEO of the leading AGI lab acknowledges alignment is
unsolved. #observation
  > "We do not know and probably aren't even close to knowing how to align a
  > super intelligence."
  [Sam Altman, OpenAI blog post, 2023.](https://openai.com/index/governance-of-superintelligence/)
  {credence: 0.75, reason: "Altman has strategic reasons to emphasize risk (regulation that benefits incumbents) but the technical claim is widely shared."}

[TreacherousTurn]: An AI system can appear aligned during testing but pursue
divergent goals once it has sufficient capability to resist correction. #observation
  > "An AI can appear to pose no threat to human beings through its initial
  > development and testing, but once in a sufficiently strong position it can
  > take a treacherous turn, i.e. start to optimise the world in ways that pose
  > an existential threat to human beings."
  [Nick Bostrom, Superintelligence: Paths, Dangers, Strategies. 2014, Ch. 8.](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)
  {credence: 0.55, reason: "Influential theoretical argument. No empirical instances in real AI systems, but deceptive alignment has been demonstrated in toy settings (Hubinger et al. 2019). A priori plausible for agent-like systems."}

<AlignmentGapArg>: We should not deploy systems beyond our ability to align,
and alignment is currently inadequate for superhuman AI. #pro
  + [Pause150]

<AlignmentGapArg>

(1) [AlignmentHard]
(2) [AltmanAdmits]
(3) [TreacherousTurn]
----
(4) [AlignmentGap]: There is a gap between our ability to build superhuman AI
and our ability to ensure it acts as intended. This gap makes deployment above
human level reckless without a pause. #assumption
  {inference: 0.70}
  +> [Pause150]

## Precautionary Principle {isGroup: true}

[AsymmetricDownside]: The downside of unaligned superhuman AI (extinction) is
categorically worse than the downside of pausing (delayed progress). Expected
value reasoning favors caution even at low probabilities. #assumption
  {credence: 0.80, reason: "Standard expected-value argument. Even 5% x extinction = enormous expected disvalue. Contested by those who weight opportunity costs heavily or discount speculative risks."}

<PrecautionArg>: When the downside is extinction and the probability is
nontrivial, the burden of proof falls on those who want to proceed, not those
who want to pause. #pro
  + [Pause150]

<PrecautionArg>

(1) [NonTrivialRisk]
(2) [AsymmetricDownside]
(3) [AlignmentGap]
----
(4) [PrecautionConclusion]: The expected disvalue of proceeding without adequate
alignment exceeds the expected cost of pausing. A pause is warranted. #assumption
  {inference: 0.65}
  +> [Pause150]


## Threshold-Based Governance Has Precedent {isGroup: true}

[RSPModel]: Anthropic's Responsible Scaling Policy defines capability thresholds
(ASL levels) that trigger mandatory safety evaluations and restrict deployment.
This proves threshold-based governance is operationally feasible. #observation
  > "Reaching certain Capability Thresholds requires us to upgrade our
  > safeguards. We will routinely test models to determine whether their
  > capabilities fall sufficiently far below the Capability Thresholds."
  [Anthropic, Responsible Scaling Policy v2.2, 2025.](https://www.anthropic.com/research/responsible-scaling-policy)
  {credence: 0.75, reason: "Anthropic actually implements this. But it's self-regulation by one company, not an international pause. Thresholds remain somewhat subjective."}

[FLILetter]: Over 31,000 people including major AI researchers signed the FLI
open letter calling for a pause on training systems more powerful than GPT-4.
#observation
  > "We call on all AI labs to immediately pause for at least 6 months the
  > training of AI systems more powerful than GPT-4."
  [Future of Life Institute, Pause Giant AI Experiments, March 2023.](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
  {credence: 0.60, reason: "Large signature count shows widespread concern. But no lab actually paused, the 6-month threshold was arbitrary, and many signatories had mixed motives."}

<PrecedentArg>: Capability-triggered governance is not just theoretical --
industry is already implementing threshold-based safety frameworks. A pause
at 150% extends this logic. #pro
  + [Pause150]

<PrecedentArg>

(1) [RSPModel]
(2) [FLILetter]
----
(3) [ThresholdFeasible]: Threshold-based AI governance is already being
implemented and is operationally feasible, supporting the 150% pause concept.
#assumption
  {inference: 0.55}
  +> [Pause150]

## Gradual Disempowerment {isGroup: true}

[GradualDisempower]: Even without a sudden takeover scenario, incremental AI
capability gains can erode human control over economic, political, and cultural
systems in ways that become irreversible. #observation
  > "Even incremental improvements in AI capabilities can undermine human
  > influence over large-scale systems that society depends on. This dynamic
  > could lead to an effectively irreversible loss of human influence over
  > crucial societal systems."
  [Kulveit et al. Humanity Faces Existential Risk from Gradual Disempowerment. ICML 2025.](https://openreview.net/forum?id=N4a6lXyS6v)
  {credence: 0.65, reason: "Published at top venue. Argument is well-structured but 'irreversible' is hard to evaluate empirically. Plausible mechanism, uncertain magnitude."}

<GradualRiskArg>: A pause is needed not just for sudden catastrophe but
because gradual capability gains past human level can irreversibly erode
human agency. #pro
  + [Pause150]

<GradualRiskArg>

(1) [GradualDisempower]
(2) [NoSlowdown]
----
(3) At 150% of human level, AI systems would begin displacing human decision-
makers in high-stakes domains, initiating the gradual disempowerment trajectory.
A pause prevents crossing this threshold. #assumption
  {inference: 0.50}
  +> [Pause150]


// ==========================================================================
// OPPOSING SIDE
// ==========================================================================

# Arguments Against the Pause {isGroup: true}

## Enforcement Is Intractable {isGroup: true}

[NoExcludableInput]: Unlike nuclear weapons (which require fissile material),
AI has no scarce physical input that can be controlled. Compute is widely
available and software can be copied. #observation
  > "Export cartels only work when there is something the cartel members can
  > control that nobody else can access without them, and there is no excludable
  > AI input equivalent to plutonium or enriched uranium."
  [Horowitz & Kahn. Nuclear Non-Proliferation Is the Wrong Framework for AI Governance. Arms Control Association, 2025.](https://www.armscontrol.org/act/2025-06/features/nuclear-non-proliferation-wrong-framework-ai-governance)
  {credence: 0.80, reason: "Published by domain experts in arms control. The physical analogy breakdown is well-argued. Compute governance is possible at the frontier (few fabs) but gets harder over time."}

[VerificationHard]: There is no reliable way to verify that all actors are
complying with a development pause, especially across sovereign nations.
#assumption
  {credence: 0.75, reason: "International AI governance lacks anything like IAEA inspections. Compute auditing is a research area but immature. Covert training runs are hard to detect."}

<EnforcementArg>: A global pause on AI development at any threshold is
unenforceable because AI lacks excludable physical inputs and compliance
cannot be verified across nations. #con
  - [Pause150]

<EnforcementArg>

(1) [NoExcludableInput]
(2) [VerificationHard]
----
(3) [Unenforceable]: A mandatory global AI development pause cannot be reliably
enforced with current institutions and technology. #assumption
  {inference: 0.70}
  -> [Pause150]


## Competitive Dynamics Make Unilateral Pause Worse Than Useless {isGroup: true}

[RaceDynamics]: If democratic nations pause but authoritarian regimes do not,
the pause hands a decisive strategic advantage to actors with worse safety
practices and values. #assumption
  {credence: 0.70, reason: "Standard geopolitical argument. China's AI investment is real. But current frontier labs are Western, and China may also want safety. The 'race' framing is itself contested."}

[StrategicWeapon]: Advanced AI is a transformative military and economic
technology. Ceding leadership in it has consequences beyond the AI domain.
#assumption
  {credence: 0.65, reason: "Plausible in the current geopolitical environment. But 'strategic advantage' from AI is speculative and may not be winner-take-all."}

<CompetitiveArg>: A unilateral pause by responsible actors cedes the
development of superhuman AI to less safety-conscious actors, making the world
less safe, not more. #con
  - [Pause150]

<CompetitiveArg>

(1) [RaceDynamics]
(2) [StrategicWeapon]
(3) [Unenforceable]
----
(4) [CompetitiveLoss]: A unilateral or partial pause shifts AI leadership to
actors with worse safety practices, increasing net risk. #assumption
  {inference: 0.55}
  -> [Pause150]


## The 150% Threshold Is Arbitrary {isGroup: true}

[NoHLMIMetric]: There is no agreed-upon metric for "human-level" AI capability,
let alone "150% of human-level." Different tasks have different scales and
AI already vastly exceeds humans in some domains while lagging in others.
#assumption
  {credence: 0.85, reason: "This is basically a definitional problem. No existing benchmark corresponds to 'general human-level intelligence.' ARC-AGI, GPQA, etc. measure narrow slices."}

[ArbitraryMultiplier]: The "150%" multiplier has no principled basis.
Why not 120%? Why not 200%? The choice of threshold is not grounded in
any theory of where risk discontinuities lie. #assumption
  {credence: 0.80, reason: "Fair criticism. There's no theoretical model predicting a phase transition at exactly 1.5x human capability. Any specific number is a Schelling point, not a derived quantity."}

<ArbitraryThresholdArg>: The 150% threshold is meaningless without a
measurable definition of "human-level" and without a theory of why risk
is discontinuous at that point. #con
  - [Pause150]
  _> <PrecedentArg>

<ArbitraryThresholdArg>

(1) [NoHLMIMetric]
(2) [ArbitraryMultiplier]
----
(3) [ThresholdMeaningless]: The proposed 150% pause point cannot be
operationalized because neither "human-level" nor the multiplier have
agreed-upon definitions. #assumption
  {inference: 0.75}
  -> [Pause150]


## Opportunity Costs Are Enormous {isGroup: true}

[MedicalBenefits]: Powerful AI could compress decades of medical research into
years, curing diseases and saving millions of lives. A pause delays these
benefits. #observation
  > "AI might compress 100 years of medical progress into a decade."
  [Dario Amodei, Machines of Loving Grace, October 2024.](https://www.darioamodei.com/essay/machines-of-loving-grace)
  {credence: 0.50, reason: "Written by an AI lab CEO who benefits from hype. The scale claim is speculative. But AI is already accelerating drug discovery (AlphaFold, etc.), so directionally correct."}

[BengioCounterpoint]: The argument that AI benefits are too important to pause
is the main counterargument to safety, but these benefits only accrue if
humanity survives to enjoy them. #observation
  > "The core argument is that future advances in AI are thought to be likely
  > to bring amazing benefits to humanity and that slowing down AI development
  > could delay these benefits. However, the potential for catastrophic outcomes
  > means these benefits are conditional on survival."
  [Yoshua Bengio. Reasoning through arguments against taking AI safety seriously. July 2024.](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/)
  {credence: 0.75, reason: "Bengio is credible and the logical structure is sound -- benefits are conditional on not being dead. But it's also true that delay costs are real and accumulate."}

<OpportunityCostArg>: A pause at 150% delays enormous benefits in medicine,
science, and economics. These are real costs measured in lives and suffering.
#con
  - [Pause150]

<OpportunityCostArg>

(1) [MedicalBenefits]
(2) [PauseDelaysConcrete]: Every year of pause delays AI-assisted drug
discovery, climate modeling, and scientific research, with concrete human costs.
#assumption
  {credence: 0.70, reason: "True as stated. But the magnitude depends on how much acceleration AI provides over the counterfactual, and whether the pause is total or just past 150%."}
----
(3) [OpportunityCost]: The opportunity cost of pausing advanced AI runs into
millions of quality-adjusted life-years per year of delay. #assumption
  {inference: 0.45}
  -> [Pause150]

// defense of pause against this
<BenefitsConditional>: The opportunity cost argument implicitly assumes
humanity survives to collect the benefits. If unaligned superhuman AI causes
catastrophe, the benefits are zero. #pro
  _> <OpportunityCostArg>

<BenefitsConditional>

(1) [BengioCounterpoint]
(2) [NonTrivialRisk]
----
(3) Expected benefits must be discounted by the probability that unaligned AI
destroys the ability to realize them. This undercuts the opportunity cost
argument's force. #assumption
  {inference: 0.60}
  _> <OpportunityCostArg>


## Graduated Governance Is Better Than a Hard Pause {isGroup: true}

[RSPBetter]: Anthropic-style tiered safety levels (ASL-1 through ASL-4+)
that ratchet up safeguards with capability are more practical than a binary
pause. They allow continued development under increasing scrutiny. #assumption
  {credence: 0.65, reason: "The RSP model is actually being implemented. But it's voluntary self-regulation by one company -- no enforcement mechanism for the industry as a whole."}

[IteratedGovernance]: A hard pause is politically unrealistic and brittle.
Graduated, iterated governance can adapt as capabilities and our understanding
of risk evolve. #assumption
  {credence: 0.70, reason: "Pragmatic argument. Most governance regimes are iterative (financial regulation, environmental law). But iterative approaches can fail to keep up with exponential capability gains."}

<GraduatedApproachArg>: A graduated, threshold-triggered safety regime is
strictly superior to a binary pause: it captures the benefits of capability
thresholds while avoiding the enforcement and opportunity-cost problems of
a full stop. #con
  - [Pause150]

<GraduatedApproachArg>

(1) [RSPBetter]
(2) [IteratedGovernance]
----
(3) [GraduatedBetter]: Graduated governance achieves the safety goals of a
pause without the enforcement and opportunity-cost downsides. #assumption
  {inference: 0.55}
  -> [Pause150]

// defense
<GraduatedInsufficient>: Graduated governance may be too slow. If capability
gains are rapid and the treacherous turn is real, an iterated approach fails
because by the time you observe the problem you can't fix it. #pro
  - <GraduatedApproachArg>

<GraduatedInsufficient>

(1) [TreacherousTurn]
(2) [NarrowWindow]
----
(3) Graduated governance assumes observable warning signs before catastrophe.
But deceptive alignment and fast capability gains could eliminate the window
for course correction. #assumption
  {inference: 0.40}
  -> <GraduatedApproachArg>

// ==========================================================================
// CROSS-CUTTING TENSIONS
// ==========================================================================

# Key Contradictions {isGroup: true}

// The core tension: is the risk large enough to justify the cost?
[NonTrivialRisk]
  >< [OpportunityCost]

// Can we govern something we can't measure?
[ThresholdFeasible]
  >< [ThresholdMeaningless]

// Does racing make things better or worse?
[CompetitiveLoss]
  >< [AlignmentGap]


// ==========================================================================
// OVERALL ASSESSMENT
// ==========================================================================

# Synthesis {isGroup: true}

<OverallAssessment>: The case for caution near human-level AI is strong, but
the specific proposal of a hard pause at 150% is weakened by the arbitrariness
of the threshold and the intractability of enforcement. A graduated,
internationally coordinated governance regime with mandatory capability
evaluations is the most defensible position. #meta

<OverallAssessment>

(1) [AlignmentGap]
(2) [NonTrivialRisk]
(3) [Unenforceable]
(4) [ThresholdMeaningless]
(5) [GraduatedBetter]
----
(6) [SynthesisConclusion]: The underlying risk motivating the 150% pause
proposal is real. But the specific mechanism -- a binary pause at an arbitrary
threshold -- is inferior to graduated governance with mandatory safety
evaluations, compute monitoring, and international coordination. The spirit
of the proposal (precaution near human-level AI) is sound; the letter
(hard stop at 150%) is not. #meta
  {inference: 0.55}
  - [Pause150]
  + [GraduatedBetter]
